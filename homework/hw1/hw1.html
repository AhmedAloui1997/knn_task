<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>
<h1 id="mcis6273-data-mining-prof.-maull-fall-2020-hw1">MCIS6273 Data Mining (Prof. Maull) / Fall 2020 / HW1</h1>
<p><strong>This assignment is worth up to 20 POINTS to your grade total if you complete it on time.</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Points <br/>Possible</th>
<th style="text-align: center;">Due Date</th>
<th style="text-align: center;">Time Commitment <br/>(estimated)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">20</td>
<td style="text-align: center;">Wednesday, Sep 23 @ Midnight</td>
<td style="text-align: center;"><em>up to</em> 20 hours</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>GRADING:</strong> Grading will be aligned with the completeness of the objectives.</p></li>
<li><p><strong>INDEPENDENT WORK:</strong> Copying, cheating, plagiarism and academic dishonesty <em>are not tolerated</em> by University or course policy. Please see the syllabus for the full departmental and University statement on the academic code of honor.</p></li>
</ul>
<h2 id="objectives">OBJECTIVES</h2>
<ul>
<li><p>Perform Exploratory Data Analysis (EDA) and apply basic statistical concepts to understand the underlying data</p></li>
<li><p>Explore PDF and CDF with Pandas and Seaborn</p></li>
<li><p>Explore distance and similarity measures in Pandas and Scikit-learn</p></li>
</ul>
<h2 id="what-to-turn-in">WHAT TO TURN IN</h2>
<p>You are being encouraged to turn the assignment in using the provided Jupyter Notebook. To do so, make a directory in your Lab environment called <code>homework/hwN</code>. Put all of your files in that directory. Then zip that directory, rename it with your name as the first part of the filename (e.g. <code>maull_hwN_files.zip</code>), then download it to your local machine, then upload the <code>.zip</code> to Blackboard.</p>
<p>If you do not know how to do this, please ask, or visit one of the many tutorials out there on the basics of using zip in Linux.</p>
<p>If you choose not to use the provided notebook, you will still need to turn in a <code>.ipynb</code> Jupyter Notebook and corresponding files according to the instructions in this homework.</p>
<h2 id="assignment-tasks">ASSIGNMENT TASKS</h2>
<h3 id="perform-exploratory-data-analysis-eda-and-apply-basic-statistical-concepts-to-understand-the-underlying-data">(35%) Perform Exploratory Data Analysis (EDA) and apply basic statistical concepts to understand the underlying data</h3>
<p>As we talked in the lecture, there are a number of basic statistical concepts we must understand in order to uncover patterns in data, from measures of centrality to measures of similarity.</p>
<p>We are going to dive right into a dataset to perform some exploratory data analysis or EDA, with our eye on uncovering some patterns in the data. We will try to apply some of the concepts we learned about the basics of the statistical notions and then move into some more interesting questions and visualizations that will help us explore further.</p>
<p>The dataset up for this task is a small dataset of diamonds that can be found on Github. You can find the dataset in it’s raw CSV form here:</p>
<ul>
<li><a href="https://git.io/JUGqS">https://git.io/JUGqS</a></li>
</ul>
<p>In it you will find nearly 54000 diamonds with 10 features – <em>cut, clarity, color, carat, depth, table, price, x, y</em> and <em>z</em>. If you would like to learn a little more about how diamonds are graded you may review this gemological resource: <a href="https://www.gemologyonline.com/diamondgrading.html">https://www.gemologyonline.com/diamondgrading.html</a> which will give you some insight into some of the broad characteristics of diamonds that gemologists inspect. While there are many controversies surrounding the way in which diamonds are extracted and priced, our goal at this point is to just explore the prices of diamonds in this dataset.</p>
<p>In this part of the assignment, we are going to explore this dataset using Pandas to initially load the CSV file and then we will dig into the data a bit more, first exploring the numerical features.</p>
<p>This is going to be fun, if not a bit challenging, as we will exercise some of the feautures of <a href=""><code>Pandas</code></a>, <a href=""><code>Numpy</code></a> and <a href=""><code>statsmodels</code></a>.</p>
<p>§ In the first task, we are going to load the data into a DataFrame using the standard <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv"><code>pd.read_csv()</code></a>. After loading the dataset, answer the following basic questions using the data:</p>
<ol type="1">
<li>What is the median price of a diamond over the entire dataset?</li>
<li>What is the mean?</li>
<li>Of the diamonds between 1.50 and 3 carats, what is the median price?</li>
<li>How many <em>Premium</em> cut diamonds with <em>VVS1</em>, <em>I1</em> and <em>IF</em> clarity are there (regardless of color)?</li>
<li>For diamonds between $3500-$7000, what is the mean carat size?</li>
</ol>
<p>§ Now that we have some warmups out of the way, let’s learn a little about <code>MultiIndex</code> in Pandas. Often we are confronted with data that may benefit from a different arrangment of the data in the DataFrame. Concretely, imagine you are building a table for the participants in a marathon. Often marathons group runners into age groups, sometimes runners are grouped by sex, and other times runners may be grouped by country and even competitive or non-competitive groups. Clearly, when looking at the data, we might like to see the data broken down into a hierarchy of those groups – we might want to traverse all Male, competitive, age 30-40 runners from Greece into a table.</p>
<p>The visual value of this cannot be overstated – clearly writing queries against such data is important, but so is being able display this data in a form that it can be communicated to others in a tabular form. This is where Pandas <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.html?highlight=multiindex#pandas.MultiIndex"><code>MultiIndex</code></a> comes into play. You will want to read up on this in order to perform the next task and to answer the following questions. Before you answer the questions, make sure your indices are as follows:</p>
<ul>
<li>in your Multindex DataFrame the row indices are nested with <em>cut</em> in the outer index, <em>color</em> in the inner index.</li>
<li>the columns will be the <em>clarity</em></li>
<li>the values for a given <code>(cut, color, clarity)</code> will the <em>mean</em> value for that combination. For example, if you were to go into the data and select all <em>Ideal cut</em>, <em>E color</em>, <em>SI1 clarity</em> diamonds and computed the <em>mean</em>, you would insert that into the table for that multi-index and column. HINT: don’t overthink this step as the multi-index, column can be accessed with something like <code>.loc[ (cut, color), clarity]</code> and you can just set the value to the mean for all the rows meeting that criterion from the origin imported table. You WILL have to create a new DataFrame – do not try to replace the original imported DataFrame in-place. This will not go well.</li>
<li>your DataFrame will look something like Figure 1 below.</li>
</ul>
<figure>
<img src="./sample_multidx_01.png" alt="" /><figcaption>Multi-index DataFrame</figcaption>
</figure>
<ol type="1">
<li>Please make sure you output the multi-index DataFrame so that I can see it</li>
</ol>
<p>§ Now that you have the DataFrame. there is one last thing to do so that you can answer some more interesting questions.</p>
<p>Pandas provides an interesting and simple way to stylize the background of your table to print it as a kind of heatmap. Based on the values, different cells will take on different colors. To save you some time, you can use the following code snippets to print out your data with a more useful color coding:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>   <span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>   cm <span class="op">=</span> sns.light_palette(<span class="st">&quot;green&quot;</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>   df_multi_idx.astype(<span class="st">&#39;float&#39;</span>).style.background_gradient(cmap<span class="op">=</span>cm)</span></code></pre></div>
<p>where <code>df_multi_idx</code> is the your multi-index DataFrame. It will look something like Figure 2 below and you will agree it is a great improvement over the original. NOTE: you are free to play with a palette other than green, since the example code is just that and you can modify it to your liking.</p>
<p>Now answer the following questions:</p>
<ol type="1">
<li>If we are looking at <em>J color, Ideal cut</em> diamonds, what seems unusual about the <em>I1 clarity</em> mean price? Recall, <em>J color</em> are considered the color right before faint yellow and are considered the least quality color rating.</li>
<li>Can you make a general observation about the average prices trends? What are you noticing, especially about the average prices as the color gets worse?</li>
<li>Which <em>cut, color</em> and <em>clarity</em> has the highest overall mean price?</li>
</ol>
<figure>
<img src="./sample_multidx_02.png" alt="" /><figcaption>Color-coded multi-index DataFrame output</figcaption>
</figure>
<p>§ Now that you have seen a trend on prices, instead of computing the mean price, compute the mean carat size of the diamond and perform the same tabular gradient enhancement. Answer the following questionsn looking at the tables you’ve generated:</p>
<ol type="1">
<li>What seems to be the trend in carat size, especially as it relates to cut and color?</li>
<li>Which cut (regardless of color) has the largest mean carat size? Is this a surprising outcome? Explain in a sentence or two what you think is going on?</li>
</ol>
<p>Plot a scatterplot of all the raw data with price on the <span class="math inline"><em>y</em></span>-axis and carats on the <span class="math inline"><em>x</em></span>-axis using this as a template:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>     your_dataframe.loc[:,[<span class="st">&#39;price&#39;</span>,<span class="st">&#39;carat&#39;</span>]].plot(kind<span class="op">=</span><span class="st">&#39;scatter&#39;</span>, x<span class="op">=</span><span class="st">&#39;carat&#39;</span>, y<span class="op">=</span><span class="st">&#39;price&#39;</span>)</span></code></pre></div>
<ol type="1">
<li>Does your plot support the claim that price increases as carats increase?</li>
</ol>
<h3 id="explore-pdf-and-cdf-with-pandas-and-seaborn">(35%) Explore PDF and CDF with Pandas and Seaborn</h3>
<p>In the readings and in the lecture there were introductory notions of frequency measures. Specifically, we are interested in concepts relating to the probability of obtaining values in a given distribution.</p>
<p>Any emperical observations can be plotted in terms of the frequency of their observations. For example, when looking at the number of customers arriving at a restuarant, we often <em>bin</em> the arrivals by hour. Doing so allows us to see how many customers come in over a range of hours allowing a business owner or analyst to prepare food, have employees available to work when the arrivals peak or put other business policies in place to provide a more pleasant experience for the customers.</p>
<p>We most often see these plots as <em>histograms</em>, and in the case of arrival times by hour, the number of bins in which we’d divide the histogram would be the number of hours the restaurant would be open and would thus show the aggregate number of customer arrivals each hour of operation.</p>
<p>Histograms naturally lead us to explore frequencies as probabilities. Indeed, we might like to understand the probability of 10 customers arriving in an hour, or the probability of more than a certain number of customers. For example, the probability of more than 20 customers arriving might tell us something interesting about the peak load of customers, etc. You will note that we have mostly been talking about <em>discrete</em> variables – that is variables that are countable and finite.</p>
<p>A typical example is the probability of the outcomes of a fair dice. The 6-sided dice has only 6 outcomes, the 8-sided, only 8 outcomes and so on. Thus, we are dealing with a discrete function and so if we let <span class="math inline"><em>X</em></span> be an event space, and <span class="math inline"><em>x</em></span> the outcome of the event (roll of a die) <span class="math inline"><em>x</em> ∈ <em>X</em> = {1, 2, 3, 4, 5, 6}</span> then the <em>probability mass function</em> is:</p>
<p><span class="math display">$$
\Pr(X=x) = \left\{
\begin{array}{lr}
\frac{1}{6} &amp; x = 1\\
\frac{1}{6} &amp; x= 2\\
\frac{1}{6} &amp; x= 3\\
\frac{1}{6} &amp; x= 4\\
\frac{1}{6} &amp; x= 5\\
\frac{1}{6} &amp; x= 6\\
\end{array}
\right.
$$</span></p>
<p>The histogram for that function is given by:</p>
<figure>
<img src="./dice_hist_240.png" alt="" /><figcaption>Histogram for fair 6-sided dice</figcaption>
</figure>
<p>And you can see rolling a fair dice takes on the same probability for all events, thus is it called a <em>discrete uniform distribution</em>.</p>
<p>The last concept is to develop is that of the of the <em>cumulative distribution function</em> or CDF. Instead of the probability of a specific value, we look at the <em>cumulative</em> probability at a given value or below. The output of the CDF the probability of obtaining that value or less or <span class="math inline">Pr (<em>X</em> ≤ <em>x</em>)</span>. As you would imagine, the CDF grows until it eventually reaches 1.0 – that is all values at or below that value have a probability of 1.</p>
<p>CDFs are interesting to explore and compare, which is exactly what we would like to do with the diamond data. In fact, by comparing CDFs, we can learn a lot about how two distributions compare with one another and it is an important tool in developing an undestanding of how the underlying data of two distinct datasets might vary.</p>
<p>To be more concrete, let’s look at the trivially simple plot of the CDF for a fair dice.</p>
<figure>
<img src="./dice_c_hist_240.png" alt="" /><figcaption>Cummulative histogram of a single fair dice</figcaption>
</figure>
<p>Similar concepts apply to continous random variables, but we’ll stick with the discrete case for this part of the assignment.</p>
<p>§ We want to explore our intuition of the diamonds through the lens of PMF and CDF, so first we’re going to use a very valuable library called <a href="https://seaborn.pydata.org/">Seaborn</a> to get a feel for the data and confirm some intuitions.</p>
<p>The first thing want to do is look at the cumulative distribution of one variables – <em>carat</em>. Our intuition is that different clarities have different carat distributions in the data. For example, we might expect that the lower the clarity, the more distrubuted the carat. Stated another way, we might find more variability in carat sizes as the clarity gets worse. Let’s look at two ends of the spectrum – the flawless (“FL”) and internally flawlss (“IF”) and the opposite end the slightly included “SI2” and “SI3” clarity diamonds. To get started, let’s take a look at the distribution of diamond clarity.</p>
<ol type="1">
<li>Produce a count of the diamond clarities using the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html?highlight=value_counts#pandas.DataFrame.value_counts"><code>pandas.DataFrame.value_counts()</code></a> method. Show the count of diamonds on a bar graph using <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html?highlight=plot#pandas.DataFrame.plot"><code>plot(kind="bar")</code></a>.</li>
<li>What observation can you make about the trend in the count of diamonds in each clarity category?</li>
<li>How do the numbers of diamonds in the “SI” and “IF/FL” groups compare?</li>
</ol>
<p>§ Now that we have an understanding of how many diamonds we have in each clarity group, let’s compare the distribution of carats for each. To do this we are going to turn to Seaborn’s <code>distplot()</code> method. Read about <code>distplot()</code> <a href="https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot">here</a>, but this template of code should help you plot what you need:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>    <span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    <span class="im">import</span> matlplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>    plt.title(<span class="st">&quot;Comparing Cumulative Distribution of Carat for I1, IF, SI1 and SI2&quot;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>    kwargs <span class="op">=</span> {<span class="st">&#39;cumulative&#39;</span>: <span class="va">True</span>}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a>    sns.distplot(the_dataframe_with_clarity_X, hist<span class="op">=</span><span class="va">False</span>, hist_kws<span class="op">=</span>kwargs, kde_kws<span class="op">=</span>kwargs, label<span class="op">=</span><span class="st">&quot;clarity_X&quot;</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a>    <span class="co"># repeat for the other clarities you want compare</span></span></code></pre></div>
<p>You now have a cumulative distribution for each clarity. You will be reminded that the <span class="math inline"><em>y</em></span>-axis shows us the cumulative probability while the <span class="math inline"><em>x</em></span>-axis shows us the carat sizes. The smoothed curve shows the density of the carat for a given probability and uses a smoothing technique called <em>kernel density estimate</em>. After displaying the plot from the code above in your notebook, answer the following questions:</p>
<ol type="1">
<li>Which color has the largest relative distribution of high carat diamonds?</li>
<li>Which has the largest relative distribution of small diamonds?</li>
<li>Based on the prior two questions, is this a surprising outcome?</li>
<li>Which two distributions look most similar?</li>
</ol>
<p>§ We’re now at the point where we’d like to graph this same data in two dimensions, with what is called the <em>kernel density estimate plot</em> or KDE plot. The KDE plot takes our bivariate data as input and outputs a smoothed 2D contour plot. You see, in the example above, we only showed a single feature of the data. While interesting, it is far more interesting when we compare two variables. Indeed, since diamonds are generally considered a luxury item, and not typically considered “inexpensive”, we might like to know how <em>carat</em> size influences <em>price</em>. This makes things a bit more interesting and useful.</p>
<p>We’re going to use the KDE plot to visually compare the densities of <em>carat</em> and <em>price</em>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>    plt.title(<span class="st">&quot;KDE Plot of Price to Carat for X and Y&quot;</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>    <span class="co"># dataset X</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>    sns.kdeplot(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>        x_carat,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>        x_price,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>        cmap<span class="op">=</span><span class="st">&quot;Reds&quot;</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>        shade<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>        shade_lowest<span class="op">=</span><span class="va">False</span>, label<span class="op">=</span><span class="st">&quot;x_label&quot;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>    <span class="co"># dataset Y</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a>    sns.kdeplot(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a>        y_carat,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a>        y_price,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>        cmap<span class="op">=</span><span class="st">&quot;Blues&quot;</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a>        shade<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a>        shade_lowest<span class="op">=</span><span class="va">False</span>, label<span class="op">=</span><span class="st">&quot;y_label&quot;</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true"></a>    plt.legend()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true"></a>    plt.savefig(<span class="st">&quot;your_plot_can_be_saved.png&quot;</span>)</span></code></pre></div>
<p>Your plot will look something like that in Figure 3 below. If you notice in the figure the densities show that SI2 has a broad distribution between a depth of 58 and 65, while the price varies greatly reaching well above $10,000. You can see IF diamonds are much tighter both in terms of price and depth.</p>
<figure>
<img src="./2d_kde_plot.png" alt="" /><figcaption>KDE Plot for depth vs. price (SI2 and IF clarity)</figcaption>
</figure>
<p>You may want to only plot each alone or no more than 2 per plot so you can see the data. After you have the plots, make sure you save them and include them in your zipped submission. Then answer the questions below based on your plots:</p>
<ol type="1">
<li>How do the density plots compare to one another? What general observation can you make about the typical SI1 and SI2 diamonds? What are their similarities?</li>
<li>What about the density plots for I1 and IF? How are they different? Please provide as much description to support your answer.</li>
<li>From the plots alone, what can you make of the price density of 1-carat SI1 diamonds versus those of SI2 diamonds?</li>
<li>What would you say of the claim: “If clarity is of great concern to you, the your best value diamonds are in found in the I1 clarity.”? What challenges could you make to this claim based on the KDE plots? What are the supports for this claim?</li>
<li>Write a query using the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html"><code>DataFrame.query()</code></a> method to show the descriptive statistics for <em>clarity I1</em> and <em>clarity SI1</em> diamonds. You can use the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe"><code>DataFrame.describe()</code></a> method to get the descriptive statistics. The query method will restrict diamonds between .95 and 1.05 carats for each color.</li>
<li>Based on that query how can it be used as evidence to support the original claim in question #4 previously?</li>
</ol>
<h3 id="explore-distance-and-similarity-measures-in-pandas-and-scikit-learn">(30%) Explore distance and similarity measures in Pandas and Scikit-learn</h3>
<p>In this last exploration we’re going to make use of the excellent distance metrics provided with Scikit-learn. In class we talked about several distance metrics, one of the most useful for numeric data being Euclidean, defined by: <span class="math inline">$D_{euclidean}(x, y) = \sqrt{ \sum_i ( x_i -y_i )^2 }$</span>.</p>
<p>We’re going to use the <code>sklearn.metrics</code> libraries to understand the similarity of certain diamonds to a reference set I will be giving you. As we go through this exercise, you’ll be building up your intuitions that will help carry you into clustering concepts and other methods of data exploration.</p>
<p>First, familiarize yourself with the <a href="https://Scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances"><code>sklearn.metrics.pairwise_distances</code></a> and <a href="https://Scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html?highlight=normalize#sklearn.preprocessing.normalize"><code>sklearn.preprocessing.normalize</code></a> methods. These will be critical for completing these tasks. You will also need to read up on the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies"><code>pandas.get_dummies()</code></a> method.</p>
<p>In class, I talked a bit about nomimal and categorical versus numeric variables. Categorical being the the variables that take on two or more named categories, like “Male” or “Female”, or color grades like “E”, “G” or “H”. Most machine learning and data mining algorithms do not deal with such variables easily as strings and thus we will often have to convert such variables to numeric or binary values, where 1 represents the presence and 0 the absence of the variable, or a scale of values like 1 to 10, etc.</p>
<p>Luckily Pandas provides this capability for us with the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies"><code>pandas.get_dummies()</code></a> method, which will automatically convert these categorical and nominal values to binary values. Sometimes this is also called <em>binarizing</em> or <em>binarization</em>. What the technique effectively does is map a series of outcome variables like color grades to numerical indicatator variables – with a 1 that the variable is present and 0 for all others. You may notice that this can create very large, and sparse datasets if you have a lot of variable outcomes.</p>
<p>You will need to load the diamonds dataset and convert the categorical variable to numeric (cut, clarity, color). Once this is done, you will be able to do what will be asked of you.</p>
<p>§ Once you have used <code>get_dummies()</code> to create a binary (numeric feature) for the variable’s presence or absence then you can perform more interesting operations such as comparing one set of data to another.</p>
<p>In the interest of being mindful of our computation restrictions on the Hub, we are going to randomly sample the data in order to develop some distance measures. There are a number of ways to get a random sample of rows from our DataFrame, but one of the easiest is the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html"><code>DataFrame.sample()</code></a> method, which will take a parameter <em>n</em> to indicate the size of the random sample. We will choose 10% of the total data or 5400 to be nice on the cloud computational resources. If you run this on your own machine, you may increase that to a much larger number, say 30%.</p>
<p>Now you will perform a final step – <em>normalization</em>. Normalization is the process of bringing data values into a common scale. Scikit-learn offers two common methods L1-norm and L2-norm. L1-norm is also known as the <em>least absolute deviations</em> or <em>least absolute errors</em> method and attempts to minimize <span class="math inline">∑<sub><em>i</em></sub>|<em>y</em><sub><em>i</em></sub> − <em>f</em>(<em>x</em><sub><em>i</em></sub>)|</span> where <span class="math inline"><em>f</em>(<em>x</em><sub><em>i</em></sub>)</span> is the estimated values for the target <span class="math inline"><em>y</em><sub><em>i</em></sub></span>. L2-norm is known as <em>least squares</em> and minimizes <span class="math inline">∑<sub><em>i</em></sub>(<em>y</em><sub><em>i</em></sub> − <em>f</em>(<em>x</em><sub><em>i</em></sub>))<sup>2</sup></span> or the sum of squares. Each method has their advantages and disadvantages, but L2-norm is the stable and computationally efficient default within the <code>sklearn.preprocessing.normalize()</code> method and so we will stay with that default in this part of the exercise.</p>
<ol type="1">
<li>Once the data is normalized and your sample has been made, please save the resulting DataFrame to a CSV file called <code>normalized_datasample_5400k.txt</code>. Also make sure the head of this dataset is also displayed in your notebook.</li>
<li>NOTE: We have sample data rows <a href="https://github.com/kmsaumcis/mcis6273_f20_datamining/tree/master/homework/hw1/sample_data.csv">here in <code>sample_data.csv</code></a> for the next questions. You might find it just as easy to insert these data into your original dataset and normalize when answering the questions, but there are other ways of doing the same thing.</li>
<li>For diamond #2 (the 0.38 carat, Ideal, G, VS1, $759), please find the 5 most similar diamonds from the sample set that you have. You will need to learn to use the <a href="https://Scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html"><code>sklearn.metrics.pairwise_distances</code></a> method obtain the distances and the <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argsort.html"><code>numpy.argsort()</code></a> method to determine the indices of the 5 closest diamonds. Use the default Euclidean metric to perform the distances calculation.</li>
<li>Run normalization again, <strong>but drop <code>price</code> from the sample before normalizing</strong>. Find the 5 closest diamonds. The price per carat for the diamond we were looking at is $1946.15. How does this compare with the price per carat of the 5 most similar diamonds? You will obviously need to keep the original dataset in order to determine the prices of the 5 most similar that you find.</li>
<li>When looking at the top 5, what commonalities do they have with the sample diamond #2? Differences?</li>
<li>Perform the same analysis on diamond #14, (1.13 carat, Ideal, F, VS2, $6283). Again how does the price per carat compare? Use the data you have as evidence to back up your answer.</li>
<li>What are the similarities / differences amongst the top 5? Please be specific in your answer.</li>
<li>Provide a reason (an intuition will do) for dropping the <code>price</code> feature from the data?</li>
</ol>
